{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FistFraudDetectionModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiE2U1NeTCak"
      },
      "source": [
        "# Creating DataFrame and Necessary Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7DqPOfrSWH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "0f588973-1951-4151-de2d-95ed0303671d"
      },
      "source": [
        "! pip install fancyimpute\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "from cycler import cycler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from fancyimpute import IterativeImputer\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fancyimpute\n",
            "  Using cached fancyimpute-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.0.31)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (3.6.4)\n",
            "Collecting nose\n",
            "  Using cached nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.2.7)\n",
            "Collecting scikit-learn>=0.24.2\n",
            "  Using cached scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "Collecting knnimpute>=0.1.0\n",
            "  Using cached knnimpute-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.4.1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (0.6.2.post0)\n",
            "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (2.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (0.70.12.2)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (2.0.7.post1)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.5.post0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy->fancyimpute) (0.3.4)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (8.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.4.0)\n",
            "Installing collected packages: threadpoolctl, scikit-learn, nose, knnimpute, fancyimpute\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed fancyimpute-0.7.0 knnimpute-0.1.0 nose-1.3.7 scikit-learn-1.0.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxpvE7GnSWzp",
        "outputId": "295c0ba3-fd67-4c7c-cdcd-483e8e5743ff"
      },
      "source": [
        "!pip install kaggle --upgrade"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zARdqN-i_hG0"
      },
      "source": [
        "### Hide Kaggle Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsqvpesPSYTP"
      },
      "source": [
        "!echo \"{\\\"username\\\":\\\"tombohorig\\\",\\\"key\\\":\\\"81f7fcd3a57da42eb3f3c20e0417fdfe\\\"}\" > kaggle.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-fO-ylwSoJp"
      },
      "source": [
        "!sudo mkdir -p ~/.kaggle\n",
        "!sudo cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0lnyg2bSp6r"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1aPpzzWSqQy",
        "outputId": "20978d9a-961c-4ae9-de72-d1dd379797d6"
      },
      "source": [
        "!kaggle --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle API 1.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyrQ-ns1SrfU",
        "outputId": "36b86242-42d7-47ee-bf81-9d5f21fcb60a"
      },
      "source": [
        "!kaggle competitions download -c ieee-fraud-detection"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test_identity.csv.zip to /content\n",
            "  0% 0.00/3.21M [00:00<?, ?B/s]\n",
            "100% 3.21M/3.21M [00:00<00:00, 53.3MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.14M [00:00<?, ?B/s]\n",
            "100% 1.14M/1.14M [00:00<00:00, 35.7MB/s]\n",
            "Downloading train_identity.csv.zip to /content\n",
            "  0% 0.00/3.26M [00:00<?, ?B/s]\n",
            "100% 3.26M/3.26M [00:00<00:00, 53.9MB/s]\n",
            "Downloading test_transaction.csv.zip to /content\n",
            " 79% 41.0M/52.2M [00:00<00:00, 60.0MB/s]\n",
            "100% 52.2M/52.2M [00:00<00:00, 76.3MB/s]\n",
            "Downloading train_transaction.csv.zip to /content\n",
            " 98% 57.0M/58.3M [00:01<00:00, 38.0MB/s]\n",
            "100% 58.3M/58.3M [00:01<00:00, 46.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ7KHEeESyvj"
      },
      "source": [
        "np.random.seed(314159)\n",
        "train_txn = pd.read_csv('/content/train_transaction.csv.zip', compression='zip')\n",
        "train_id = pd.read_csv('/content/train_identity.csv.zip', compression='zip')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRPGxHMsS7qO",
        "outputId": "10f35e98-a676-434e-fe69-62d049412196"
      },
      "source": [
        "train_txn.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 394)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pPI5mqSS8Vm",
        "outputId": "e0701327-5b28-4220-bd66-ae5f936c3ecf"
      },
      "source": [
        "train_id.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(144233, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtK9phbuuw9q"
      },
      "source": [
        "# Cleaning Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSuRhQaUuzcQ"
      },
      "source": [
        "### Drop columns with null values over a certain threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnULDOynu9Cz"
      },
      "source": [
        "def remove_heavy_null_columns(df, threshold, n_rows):\n",
        "  cols_over_threshold = []\n",
        "  for column in df:\n",
        "    missing_values = df[column].isna().sum()\n",
        "    if missing_values != 0 and missing_values/n_rows >= threshold:\n",
        "      cols_over_threshold.append(column)\n",
        "  df.drop(cols_over_threshold, axis=1, inplace=True)\n",
        "\n",
        "threshold = 0.5\n",
        "remove_heavy_null_columns(df=train_txn, threshold=threshold, n_rows=train_txn.shape[0])\n",
        "remove_heavy_null_columns(df=train_id, threshold=threshold, n_rows=train_txn.shape[0])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjg_5AQVy09E"
      },
      "source": [
        "### Perform MICE Imputation on remaining Nan Cells\n",
        "After deciding to use a XGBoost Model and learning that it can deal with Nan Values and learn from them I will not impute these values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFHneSYDzh9u"
      },
      "source": [
        "# def fill_missing_values(df):\n",
        "#   # get all column names:\n",
        "#   print(df.head(20))\n",
        "#   column_names = df.columns\n",
        "#   # mice = IterativeImputer()\n",
        "#   # return pd.DataFrame(mice.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# # print(\"BEFORE IMPUTATION:\")\n",
        "# # print(train_id.isna().sum())\n",
        "# # # print(penguins.isna().sum())\n",
        "\n",
        "# fill_missing_values(df=train_id)\n",
        "\n",
        "# # print()\n",
        "# # print(\"AFTER IMPUTATION:\")\n",
        "# # print(train_id.isna().sum())\n",
        "# # # print(penguins.isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfyC8IiX8vt1"
      },
      "source": [
        "# Convert columns with String to be One Hot Encoded as converting to numeric in same column will give undesired order precedence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-7ABY5cGNOZ"
      },
      "source": [
        "# One hot encodings:\n",
        "def one_hot_encoding(df, column_name):\n",
        "  encoding = pd.get_dummies(df[column_name])\n",
        "  df = df.drop(column_name, axis = 1)\n",
        "  df = df.join(encoding)  \n",
        "  return df"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZgkWUP3GZNo"
      },
      "source": [
        "Preform Cleaning on train_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bHHMgw2_4D1"
      },
      "source": [
        "# Create maps for cols that should be one hot encoded, but I run out of RAM:\n",
        "def get_mapped_vals(df, feature):\n",
        "  unique_vals = df[feature].unique()\n",
        "  map = {}\n",
        "  for i in range(len(unique_vals) ):\n",
        "    map[unique_vals[i] ] = i\n",
        "  return map"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4DE67xR85Hs",
        "outputId": "51480414-218a-4442-8fb9-5b0c3826cb00"
      },
      "source": [
        "# Transormations:\n",
        "boolean_found = {'NotFound': 0, 'Found': 1, None: 0}\n",
        "boolean_new = {'New': 0, 'Found': 1, None: 0}\n",
        "true_false = {'T': 2, 'F': 1, None: 0}\n",
        "device_type = {None: 0, 'mobile': 1, 'desktop': 2}\n",
        "train_id['id_12'].replace(boolean_found, inplace=True)\n",
        "train_id['id_16'].replace(boolean_found, inplace=True)\n",
        "train_id['id_27'].replace(boolean_found, inplace=True)\n",
        "train_id['id_28'].replace(boolean_new, inplace=True)\n",
        "train_id['id_29'].replace(boolean_found, inplace=True)\n",
        "train_id['id_35'].replace(true_false, inplace=True)\n",
        "train_id['id_36'].replace(true_false, inplace=True)\n",
        "train_id['id_37'].replace(true_false, inplace=True)\n",
        "train_id['id_38'].replace(true_false, inplace=True)\n",
        "train_id['DeviceType'].replace(device_type, inplace=True)\n",
        "\n",
        "# One Hot Encodings:\n",
        "train_id = one_hot_encoding(df=train_id, column_name='id_15')\n",
        "train_id = one_hot_encoding(df=train_id, column_name='id_23')\n",
        "train_id = one_hot_encoding(df=train_id, column_name='id_34')\n",
        "\n",
        "# I must drop transfrom instead of one-hot encode b/c I run out of RAM:\n",
        "train_id['id_30'].replace(get_mapped_vals(df=train_id, feature='id_30'), inplace=True)\n",
        "train_id['id_31'].replace(get_mapped_vals(df=train_id, feature='id_31'), inplace=True)\n",
        "train_id['id_33'].replace(get_mapped_vals(df=train_id, feature='id_33'), inplace=True)\n",
        "train_id['DeviceInfo'].replace(get_mapped_vals(df=train_id, feature='DeviceInfo'), inplace=True)\n",
        "\n",
        "\n",
        "train_id.head(10)\n",
        "print(train_id.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144233, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYqyn9i5GjRk"
      },
      "source": [
        "Preform Cleaning on Transactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DanpuWFMGlf3",
        "outputId": "0f85897c-5608-4aec-edb2-0dd8cebcf374"
      },
      "source": [
        "# Transormations:\n",
        "true_false = {'T': 2, 'F': 1, None: 0}\n",
        "train_txn['M1'].replace(true_false, inplace=True)\n",
        "train_txn['M2'].replace(true_false, inplace=True)\n",
        "train_txn['M3'].replace(true_false, inplace=True)\n",
        "train_txn['M6'].replace(true_false, inplace=True)\n",
        "\n",
        "# One Hot Encoding:\n",
        "train_txn = one_hot_encoding(df=train_txn, column_name='ProductCD')\n",
        "train_txn = one_hot_encoding(df=train_txn, column_name='card4')\n",
        "train_txn = one_hot_encoding(df=train_txn, column_name='card6')\n",
        "train_txn = one_hot_encoding(df=train_txn, column_name='P_emaildomain')\n",
        "\n",
        "train_txn['M4'] = 'M4 ' + train_txn['M4'].astype(str)\n",
        "train_txn = one_hot_encoding(df=train_txn, column_name='M4')\n",
        "\n",
        "train_txn.head(10)\n",
        "train_txn.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 291)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjhzRGSk4_G_"
      },
      "source": [
        "# XGBoost Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VROb4r4o5H7Y"
      },
      "source": [
        "### Combine the models together and train/test split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAg-Iq5s5KE0",
        "outputId": "4f7df36b-dec1-46ed-9508-e47380360b2e"
      },
      "source": [
        "# I've chosen an outer join b/c we previously discusses that it is better to not reduce the rows to the size of train_id but to keep the rows from train_txn:\n",
        "df = pd.merge(train_txn, train_id, on='TransactionID', how='inner')\n",
        "df.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(144233, 338)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuc7sDuY5xwu"
      },
      "source": [
        "# Train/Test Split:\n",
        "X = df.drop('isFraud', axis=1)\n",
        "y = df['isFraud']\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA2jSgNL5p4x"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvrWNC69_TVN",
        "outputId": "906699fd-e06f-4610-fee3-f66a6523e6d7"
      },
      "source": [
        "# Create model:\n",
        "xgb_clf = XGBClassifier()\n",
        "xgb_clf.fit(train_x, train_y, eval_metric='aucpr')\n",
        "# Print the accuracy score\n",
        "print(xgb_clf.score(test_x, test_y))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9563559468922245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0GGjCn_Bf1j",
        "outputId": "ac01ca39-4c52-4201-ca93-17e5dcfff581"
      },
      "source": [
        "# Check if better then just guessing no every time:\n",
        "not_fraud, fraud = df['isFraud'].value_counts()\n",
        "fraud_propotion = fraud/not_fraud\n",
        "print(f\"The proportion of fraud is {fraud_propotion}.\")\n",
        "print(f\"My model is {0.9565639407910701-(1-fraud_propotion)} percent better than guessing yes every time.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The proportion of fraud is 0.08515216491742843.\n",
            "My model is 0.041716105708498485 percent better than guessing yes every time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFr290Y-oHk"
      },
      "source": [
        "### Get metrics on model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m55zFpYCSIG",
        "outputId": "f9b20184-c82d-4304-b588-3d3253fe74d7"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted_proababilitis = xgb_clf.predict_proba(test_x)\n",
        "precictions = xgb_clf.predict(test_x)\n",
        "print(classification_report(test_y, precictions) )\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     26596\n",
            "           1       0.88      0.51      0.65      2251\n",
            "\n",
            "    accuracy                           0.96     28847\n",
            "   macro avg       0.92      0.75      0.81     28847\n",
            "weighted avg       0.95      0.96      0.95     28847\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKtywuVrE0fz"
      },
      "source": [
        "This shows that the recall takes a big hit when trying to identify fraudelent transactions. I believe a wasy to better tackle this issue is to use SMOTE to tackle the class imbalance. Once the classes are balanced the model's recall and even precision should rise in fraudulent transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jCPgurqFL5t"
      },
      "source": [
        "### Use SMOTE to resolve class imbalance\n",
        "\n",
        "> This will be attempted in next stage of modelling as this is only the initial model stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY6mwRHlFQ5s"
      },
      "source": [
        "# from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-HHPoi_FSXv"
      },
      "source": [
        "# # Train/Test Split:\n",
        "# X = df.drop('isFraud', axis=1)\n",
        "# y = df['isFraud']\n",
        "\n",
        "# # To be able to use SMOTE we must also impute all the values:\n",
        "# # Th\n",
        "# # Use SMOTE here\n",
        "# oversample = SMOTE()\n",
        "# X, y = oversample.fit_resample(X, y)\n",
        "\n",
        "# train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 40,
      "outputs": []
    }
  ]
}