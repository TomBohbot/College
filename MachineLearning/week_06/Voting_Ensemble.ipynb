{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voting_Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k50WCqlapU7W"
      },
      "source": [
        "# The goal is to ensemble the best logistic regression model, knn classifier, and decision tree via a Hard Voting Classifier:"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuNeLrZcr97l"
      },
      "source": [
        "# Required import statements:\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from google.colab import files"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg-5qEU6pnfM"
      },
      "source": [
        "# Keras Logarithmic Regression Code From week_04 Condensed in One Cell:\n",
        "\n",
        "def logarithmic_regression_keras(train_x, train_y, test_x, test_y):\n",
        "\n",
        "  # Build the model:\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_shape=(2,), activation='relu', name='fc1', bias_regularizer=l2(0.01)))\n",
        "  model.add(Dense(10, activation='relu', name='fc2', bias_regularizer=l2(0.01)))\n",
        "  model.add(Dense(3, activation='softmax', name='output', bias_regularizer=l2(0.01)))\n",
        "\n",
        "  # Adam optimizer with learning rate of 0.001\n",
        "  optimizer = Adam(lr=0.001)\n",
        "  model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
        "\n",
        "  # Test on unseen data\n",
        "  results = model.evaluate(test_x, test_y)\n",
        "\n",
        "  # Find the individual outputs of the method:\n",
        "  ind_outputs = model.predict(test_x)\n",
        "  results = []\n",
        "  for flower in ind_outputs:\n",
        "    if flower[0] == max(flower):\n",
        "      results.append(0)\n",
        "    if flower[1] == max(flower):\n",
        "      results.append(1)\n",
        "    if flower[2] == max(flower):\n",
        "      results.append(2)\n",
        "  return results"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXat9hsJqM9H"
      },
      "source": [
        "# knn code pasted and condensed in one cell:\n",
        "\n",
        "# Helper method to implement Knn:\n",
        "def distance(x1, y1, x2, y2):\n",
        "  return np.sqrt( (x1-x2)**2 + (y1-y2)**2)\n",
        "\n",
        "def implement_knn(k, x, y, train_x, train_y):\n",
        "  closestPoints = []\n",
        "  for i in range(len(train_x)):\n",
        "    distancee = distance(x, y, train_x[i][0], train_x[i][1])\n",
        "    closestPoints.append([distancee, train_y[i] ])\n",
        "  sortedDistances = sorted(closestPoints, key=lambda x: x[0])\n",
        "  # Calculate which element is closest:\n",
        "  setosa = 0\n",
        "  versicolor = 0\n",
        "  virginica = 0\n",
        "  for i in range(k):\n",
        "    if sortedDistances[i][1] == 0:\n",
        "      setosa += 1\n",
        "    elif sortedDistances[i][1] == 1:\n",
        "      versicolor += 1\n",
        "    elif sortedDistances[i][1] == 2:\n",
        "      virginica += 1\n",
        "  if setosa >= versicolor and setosa >= virginica:\n",
        "    return 0\n",
        "  elif versicolor >= setosa and versicolor >= virginica:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "\n",
        "# Produce output of entire array:\n",
        "def knn_predictions(test_x, train_x, train_y):\n",
        "  knn_predictions = []\n",
        "  for i in range(len(test_x) ):\n",
        "    knn_predictions.append(implement_knn(5, test_x[i][0], test_x[i][1], train_x, train_y) )\n",
        "  return knn_predictions"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_l1xwo8Cu8s"
      },
      "source": [
        "# Decision Tree Code:\n",
        "def dec_tree(x, y):\n",
        "  tree_clf = DecisionTreeClassifier(max_depth=5) # Textbook uses 2 as a max depth.\n",
        "  tree_clf.fit(x,y) # Fit data to tree\n",
        "  predictions = tree_clf.predict(test_x)\n",
        "  tree_predictions = []\n",
        "  for i in range(len(predictions)):\n",
        "    tree_predictions.append(predictions[i])\n",
        "  return tree_predictions"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ENV6jOp-6E"
      },
      "source": [
        "# load the iris dataset: \n",
        "iris_data = load_iris()\n",
        "\n",
        "# Set x as flower characteristics and y as target values:\n",
        "x = iris_data.data[:,[2,3] ]            # Use only 2 characteristics from the iris dataset.\n",
        "y = iris_data.target \n",
        "\n",
        "# Split the data for training and testing:\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)\n",
        "\n",
        "# One hot encode some of the code for the logaritmic regression code:\n",
        "train_y_log = np.eye(3)[train_y]\n",
        "test_y_log = np.eye(3)[test_y]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xr1UftGD9KM"
      },
      "source": [
        "# Voting classifier:\n",
        "def voting_classifier():\n",
        "  log_reg_results = logarithmic_regression_keras(train_x, train_y_log, test_x, test_y_log)\n",
        "  knn_results = knn_predictions(test_x, train_x, train_y)\n",
        "  dec_tree_results = dec_tree(test_x, test_y)\n",
        "  voting_predictions= []\n",
        "  for i in range(len(test_y) ):\n",
        "    results = [knn_results[i], log_reg_results[i], dec_tree_results[i] ]\n",
        "    if knn_results[i] == log_reg_results[i] or  knn_results[i] == dec_tree_results[i]:\n",
        "      voting_predictions.append(knn_results[i])\n",
        "    elif log_reg_results[i] == dec_tree_results[i]:\n",
        "      voting_predictions.append(log_reg_results[i])\n",
        "    else:\n",
        "      # No most freq output:\n",
        "      voting_predictions.append(-1)\n",
        "  return voting_predictions\n",
        "\n",
        "# Determine accuracy of voting classifier:\n",
        "def voting_accuracy(voting_predictions, test_y):\n",
        "  correct = 0\n",
        "  for i in range(len(voting_predictions) ):\n",
        "    if voting_predictions[i] == test_y[i]:\n",
        "      correct += 1\n",
        "  accuracy = correct/len(test_y)\n",
        "  print(\"Voting Classifier Accuracy:\", accuracy)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvJSOfqMEzYV",
        "outputId": "9b7a5951-7d18-45f2-d50f-5eca480b3fa2"
      },
      "source": [
        "voting_predictions = voting_classifier()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "24/24 - 1s - loss: 1.2325 - accuracy: 0.3500\n",
            "Epoch 2/200\n",
            "24/24 - 0s - loss: 1.1452 - accuracy: 0.1417\n",
            "Epoch 3/200\n",
            "24/24 - 0s - loss: 1.0985 - accuracy: 0.2917\n",
            "Epoch 4/200\n",
            "24/24 - 0s - loss: 1.0614 - accuracy: 0.3750\n",
            "Epoch 5/200\n",
            "24/24 - 0s - loss: 1.0346 - accuracy: 0.5583\n",
            "Epoch 6/200\n",
            "24/24 - 0s - loss: 1.0111 - accuracy: 0.6333\n",
            "Epoch 7/200\n",
            "24/24 - 0s - loss: 0.9839 - accuracy: 0.6833\n",
            "Epoch 8/200\n",
            "24/24 - 0s - loss: 0.9555 - accuracy: 0.6917\n",
            "Epoch 9/200\n",
            "24/24 - 0s - loss: 0.9229 - accuracy: 0.6917\n",
            "Epoch 10/200\n",
            "24/24 - 0s - loss: 0.8937 - accuracy: 0.6917\n",
            "Epoch 11/200\n",
            "24/24 - 0s - loss: 0.8588 - accuracy: 0.6917\n",
            "Epoch 12/200\n",
            "24/24 - 0s - loss: 0.8251 - accuracy: 0.7000\n",
            "Epoch 13/200\n",
            "24/24 - 0s - loss: 0.7929 - accuracy: 0.7083\n",
            "Epoch 14/200\n",
            "24/24 - 0s - loss: 0.7586 - accuracy: 0.7167\n",
            "Epoch 15/200\n",
            "24/24 - 0s - loss: 0.7264 - accuracy: 0.7083\n",
            "Epoch 16/200\n",
            "24/24 - 0s - loss: 0.6938 - accuracy: 0.7500\n",
            "Epoch 17/200\n",
            "24/24 - 0s - loss: 0.6623 - accuracy: 0.7417\n",
            "Epoch 18/200\n",
            "24/24 - 0s - loss: 0.6304 - accuracy: 0.7667\n",
            "Epoch 19/200\n",
            "24/24 - 0s - loss: 0.6029 - accuracy: 0.7500\n",
            "Epoch 20/200\n",
            "24/24 - 0s - loss: 0.5748 - accuracy: 0.8417\n",
            "Epoch 21/200\n",
            "24/24 - 0s - loss: 0.5507 - accuracy: 0.8000\n",
            "Epoch 22/200\n",
            "24/24 - 0s - loss: 0.5250 - accuracy: 0.8583\n",
            "Epoch 23/200\n",
            "24/24 - 0s - loss: 0.5029 - accuracy: 0.8500\n",
            "Epoch 24/200\n",
            "24/24 - 0s - loss: 0.4812 - accuracy: 0.8667\n",
            "Epoch 25/200\n",
            "24/24 - 0s - loss: 0.4620 - accuracy: 0.8833\n",
            "Epoch 26/200\n",
            "24/24 - 0s - loss: 0.4507 - accuracy: 0.8667\n",
            "Epoch 27/200\n",
            "24/24 - 0s - loss: 0.4311 - accuracy: 0.9167\n",
            "Epoch 28/200\n",
            "24/24 - 0s - loss: 0.4162 - accuracy: 0.8667\n",
            "Epoch 29/200\n",
            "24/24 - 0s - loss: 0.3994 - accuracy: 0.8750\n",
            "Epoch 30/200\n",
            "24/24 - 0s - loss: 0.3884 - accuracy: 0.9083\n",
            "Epoch 31/200\n",
            "24/24 - 0s - loss: 0.3783 - accuracy: 0.8750\n",
            "Epoch 32/200\n",
            "24/24 - 0s - loss: 0.3658 - accuracy: 0.9000\n",
            "Epoch 33/200\n",
            "24/24 - 0s - loss: 0.3556 - accuracy: 0.9083\n",
            "Epoch 34/200\n",
            "24/24 - 0s - loss: 0.3474 - accuracy: 0.9083\n",
            "Epoch 35/200\n",
            "24/24 - 0s - loss: 0.3395 - accuracy: 0.9333\n",
            "Epoch 36/200\n",
            "24/24 - 0s - loss: 0.3287 - accuracy: 0.9250\n",
            "Epoch 37/200\n",
            "24/24 - 0s - loss: 0.3245 - accuracy: 0.9333\n",
            "Epoch 38/200\n",
            "24/24 - 0s - loss: 0.3196 - accuracy: 0.8917\n",
            "Epoch 39/200\n",
            "24/24 - 0s - loss: 0.3141 - accuracy: 0.9250\n",
            "Epoch 40/200\n",
            "24/24 - 0s - loss: 0.3033 - accuracy: 0.9417\n",
            "Epoch 41/200\n",
            "24/24 - 0s - loss: 0.2957 - accuracy: 0.9333\n",
            "Epoch 42/200\n",
            "24/24 - 0s - loss: 0.2951 - accuracy: 0.9250\n",
            "Epoch 43/200\n",
            "24/24 - 0s - loss: 0.2871 - accuracy: 0.9250\n",
            "Epoch 44/200\n",
            "24/24 - 0s - loss: 0.2843 - accuracy: 0.9333\n",
            "Epoch 45/200\n",
            "24/24 - 0s - loss: 0.2775 - accuracy: 0.9417\n",
            "Epoch 46/200\n",
            "24/24 - 0s - loss: 0.2740 - accuracy: 0.9417\n",
            "Epoch 47/200\n",
            "24/24 - 0s - loss: 0.2692 - accuracy: 0.9417\n",
            "Epoch 48/200\n",
            "24/24 - 0s - loss: 0.2643 - accuracy: 0.9417\n",
            "Epoch 49/200\n",
            "24/24 - 0s - loss: 0.2633 - accuracy: 0.9333\n",
            "Epoch 50/200\n",
            "24/24 - 0s - loss: 0.2674 - accuracy: 0.9333\n",
            "Epoch 51/200\n",
            "24/24 - 0s - loss: 0.2560 - accuracy: 0.9417\n",
            "Epoch 52/200\n",
            "24/24 - 0s - loss: 0.2541 - accuracy: 0.9417\n",
            "Epoch 53/200\n",
            "24/24 - 0s - loss: 0.2538 - accuracy: 0.9500\n",
            "Epoch 54/200\n",
            "24/24 - 0s - loss: 0.2491 - accuracy: 0.9417\n",
            "Epoch 55/200\n",
            "24/24 - 0s - loss: 0.2435 - accuracy: 0.9417\n",
            "Epoch 56/200\n",
            "24/24 - 0s - loss: 0.2414 - accuracy: 0.9417\n",
            "Epoch 57/200\n",
            "24/24 - 0s - loss: 0.2379 - accuracy: 0.9500\n",
            "Epoch 58/200\n",
            "24/24 - 0s - loss: 0.2378 - accuracy: 0.9417\n",
            "Epoch 59/200\n",
            "24/24 - 0s - loss: 0.2485 - accuracy: 0.9333\n",
            "Epoch 60/200\n",
            "24/24 - 0s - loss: 0.2274 - accuracy: 0.9417\n",
            "Epoch 61/200\n",
            "24/24 - 0s - loss: 0.2402 - accuracy: 0.9333\n",
            "Epoch 62/200\n",
            "24/24 - 0s - loss: 0.2338 - accuracy: 0.9500\n",
            "Epoch 63/200\n",
            "24/24 - 0s - loss: 0.2305 - accuracy: 0.9333\n",
            "Epoch 64/200\n",
            "24/24 - 0s - loss: 0.2333 - accuracy: 0.9417\n",
            "Epoch 65/200\n",
            "24/24 - 0s - loss: 0.2303 - accuracy: 0.9250\n",
            "Epoch 66/200\n",
            "24/24 - 0s - loss: 0.2240 - accuracy: 0.9500\n",
            "Epoch 67/200\n",
            "24/24 - 0s - loss: 0.2246 - accuracy: 0.9417\n",
            "Epoch 68/200\n",
            "24/24 - 0s - loss: 0.2228 - accuracy: 0.9417\n",
            "Epoch 69/200\n",
            "24/24 - 0s - loss: 0.2209 - accuracy: 0.9333\n",
            "Epoch 70/200\n",
            "24/24 - 0s - loss: 0.2255 - accuracy: 0.9417\n",
            "Epoch 71/200\n",
            "24/24 - 0s - loss: 0.2156 - accuracy: 0.9417\n",
            "Epoch 72/200\n",
            "24/24 - 0s - loss: 0.2175 - accuracy: 0.9500\n",
            "Epoch 73/200\n",
            "24/24 - 0s - loss: 0.2149 - accuracy: 0.9500\n",
            "Epoch 74/200\n",
            "24/24 - 0s - loss: 0.2151 - accuracy: 0.9333\n",
            "Epoch 75/200\n",
            "24/24 - 0s - loss: 0.2179 - accuracy: 0.9417\n",
            "Epoch 76/200\n",
            "24/24 - 0s - loss: 0.2117 - accuracy: 0.9417\n",
            "Epoch 77/200\n",
            "24/24 - 0s - loss: 0.2142 - accuracy: 0.9333\n",
            "Epoch 78/200\n",
            "24/24 - 0s - loss: 0.2093 - accuracy: 0.9500\n",
            "Epoch 79/200\n",
            "24/24 - 0s - loss: 0.2126 - accuracy: 0.9417\n",
            "Epoch 80/200\n",
            "24/24 - 0s - loss: 0.2038 - accuracy: 0.9417\n",
            "Epoch 81/200\n",
            "24/24 - 0s - loss: 0.2070 - accuracy: 0.9417\n",
            "Epoch 82/200\n",
            "24/24 - 0s - loss: 0.2043 - accuracy: 0.9417\n",
            "Epoch 83/200\n",
            "24/24 - 0s - loss: 0.2057 - accuracy: 0.9500\n",
            "Epoch 84/200\n",
            "24/24 - 0s - loss: 0.2040 - accuracy: 0.9500\n",
            "Epoch 85/200\n",
            "24/24 - 0s - loss: 0.2055 - accuracy: 0.9417\n",
            "Epoch 86/200\n",
            "24/24 - 0s - loss: 0.2044 - accuracy: 0.9500\n",
            "Epoch 87/200\n",
            "24/24 - 0s - loss: 0.2092 - accuracy: 0.9333\n",
            "Epoch 88/200\n",
            "24/24 - 0s - loss: 0.2028 - accuracy: 0.9500\n",
            "Epoch 89/200\n",
            "24/24 - 0s - loss: 0.1997 - accuracy: 0.9500\n",
            "Epoch 90/200\n",
            "24/24 - 0s - loss: 0.2011 - accuracy: 0.9417\n",
            "Epoch 91/200\n",
            "24/24 - 0s - loss: 0.1996 - accuracy: 0.9500\n",
            "Epoch 92/200\n",
            "24/24 - 0s - loss: 0.1952 - accuracy: 0.9417\n",
            "Epoch 93/200\n",
            "24/24 - 0s - loss: 0.2009 - accuracy: 0.9333\n",
            "Epoch 94/200\n",
            "24/24 - 0s - loss: 0.1989 - accuracy: 0.9500\n",
            "Epoch 95/200\n",
            "24/24 - 0s - loss: 0.1969 - accuracy: 0.9500\n",
            "Epoch 96/200\n",
            "24/24 - 0s - loss: 0.1979 - accuracy: 0.9500\n",
            "Epoch 97/200\n",
            "24/24 - 0s - loss: 0.1957 - accuracy: 0.9417\n",
            "Epoch 98/200\n",
            "24/24 - 0s - loss: 0.1952 - accuracy: 0.9500\n",
            "Epoch 99/200\n",
            "24/24 - 0s - loss: 0.1936 - accuracy: 0.9417\n",
            "Epoch 100/200\n",
            "24/24 - 0s - loss: 0.1935 - accuracy: 0.9500\n",
            "Epoch 101/200\n",
            "24/24 - 0s - loss: 0.1949 - accuracy: 0.9500\n",
            "Epoch 102/200\n",
            "24/24 - 0s - loss: 0.1974 - accuracy: 0.9417\n",
            "Epoch 103/200\n",
            "24/24 - 0s - loss: 0.1948 - accuracy: 0.9500\n",
            "Epoch 104/200\n",
            "24/24 - 0s - loss: 0.1958 - accuracy: 0.9500\n",
            "Epoch 105/200\n",
            "24/24 - 0s - loss: 0.2020 - accuracy: 0.9417\n",
            "Epoch 106/200\n",
            "24/24 - 0s - loss: 0.1944 - accuracy: 0.9500\n",
            "Epoch 107/200\n",
            "24/24 - 0s - loss: 0.1917 - accuracy: 0.9417\n",
            "Epoch 108/200\n",
            "24/24 - 0s - loss: 0.1907 - accuracy: 0.9417\n",
            "Epoch 109/200\n",
            "24/24 - 0s - loss: 0.1900 - accuracy: 0.9500\n",
            "Epoch 110/200\n",
            "24/24 - 0s - loss: 0.1901 - accuracy: 0.9500\n",
            "Epoch 111/200\n",
            "24/24 - 0s - loss: 0.1875 - accuracy: 0.9500\n",
            "Epoch 112/200\n",
            "24/24 - 0s - loss: 0.1893 - accuracy: 0.9417\n",
            "Epoch 113/200\n",
            "24/24 - 0s - loss: 0.1877 - accuracy: 0.9417\n",
            "Epoch 114/200\n",
            "24/24 - 0s - loss: 0.1870 - accuracy: 0.9500\n",
            "Epoch 115/200\n",
            "24/24 - 0s - loss: 0.1877 - accuracy: 0.9417\n",
            "Epoch 116/200\n",
            "24/24 - 0s - loss: 0.1869 - accuracy: 0.9500\n",
            "Epoch 117/200\n",
            "24/24 - 0s - loss: 0.1888 - accuracy: 0.9417\n",
            "Epoch 118/200\n",
            "24/24 - 0s - loss: 0.1859 - accuracy: 0.9500\n",
            "Epoch 119/200\n",
            "24/24 - 0s - loss: 0.1860 - accuracy: 0.9500\n",
            "Epoch 120/200\n",
            "24/24 - 0s - loss: 0.1954 - accuracy: 0.9417\n",
            "Epoch 121/200\n",
            "24/24 - 0s - loss: 0.1802 - accuracy: 0.9500\n",
            "Epoch 122/200\n",
            "24/24 - 0s - loss: 0.1841 - accuracy: 0.9417\n",
            "Epoch 123/200\n",
            "24/24 - 0s - loss: 0.1863 - accuracy: 0.9500\n",
            "Epoch 124/200\n",
            "24/24 - 0s - loss: 0.1849 - accuracy: 0.9417\n",
            "Epoch 125/200\n",
            "24/24 - 0s - loss: 0.1908 - accuracy: 0.9500\n",
            "Epoch 126/200\n",
            "24/24 - 0s - loss: 0.1834 - accuracy: 0.9500\n",
            "Epoch 127/200\n",
            "24/24 - 0s - loss: 0.1898 - accuracy: 0.9333\n",
            "Epoch 128/200\n",
            "24/24 - 0s - loss: 0.1811 - accuracy: 0.9500\n",
            "Epoch 129/200\n",
            "24/24 - 0s - loss: 0.1834 - accuracy: 0.9500\n",
            "Epoch 130/200\n",
            "24/24 - 0s - loss: 0.1822 - accuracy: 0.9500\n",
            "Epoch 131/200\n",
            "24/24 - 0s - loss: 0.1869 - accuracy: 0.9417\n",
            "Epoch 132/200\n",
            "24/24 - 0s - loss: 0.1800 - accuracy: 0.9500\n",
            "Epoch 133/200\n",
            "24/24 - 0s - loss: 0.1808 - accuracy: 0.9500\n",
            "Epoch 134/200\n",
            "24/24 - 0s - loss: 0.1813 - accuracy: 0.9500\n",
            "Epoch 135/200\n",
            "24/24 - 0s - loss: 0.1803 - accuracy: 0.9500\n",
            "Epoch 136/200\n",
            "24/24 - 0s - loss: 0.1801 - accuracy: 0.9500\n",
            "Epoch 137/200\n",
            "24/24 - 0s - loss: 0.1783 - accuracy: 0.9417\n",
            "Epoch 138/200\n",
            "24/24 - 0s - loss: 0.1789 - accuracy: 0.9500\n",
            "Epoch 139/200\n",
            "24/24 - 0s - loss: 0.1889 - accuracy: 0.9500\n",
            "Epoch 140/200\n",
            "24/24 - 0s - loss: 0.1806 - accuracy: 0.9500\n",
            "Epoch 141/200\n",
            "24/24 - 0s - loss: 0.1809 - accuracy: 0.9500\n",
            "Epoch 142/200\n",
            "24/24 - 0s - loss: 0.1838 - accuracy: 0.9417\n",
            "Epoch 143/200\n",
            "24/24 - 0s - loss: 0.1834 - accuracy: 0.9500\n",
            "Epoch 144/200\n",
            "24/24 - 0s - loss: 0.1812 - accuracy: 0.9500\n",
            "Epoch 145/200\n",
            "24/24 - 0s - loss: 0.1764 - accuracy: 0.9417\n",
            "Epoch 146/200\n",
            "24/24 - 0s - loss: 0.1802 - accuracy: 0.9417\n",
            "Epoch 147/200\n",
            "24/24 - 0s - loss: 0.1733 - accuracy: 0.9500\n",
            "Epoch 148/200\n",
            "24/24 - 0s - loss: 0.1802 - accuracy: 0.9500\n",
            "Epoch 149/200\n",
            "24/24 - 0s - loss: 0.1796 - accuracy: 0.9500\n",
            "Epoch 150/200\n",
            "24/24 - 0s - loss: 0.1797 - accuracy: 0.9500\n",
            "Epoch 151/200\n",
            "24/24 - 0s - loss: 0.1779 - accuracy: 0.9500\n",
            "Epoch 152/200\n",
            "24/24 - 0s - loss: 0.1742 - accuracy: 0.9500\n",
            "Epoch 153/200\n",
            "24/24 - 0s - loss: 0.1792 - accuracy: 0.9500\n",
            "Epoch 154/200\n",
            "24/24 - 0s - loss: 0.1740 - accuracy: 0.9500\n",
            "Epoch 155/200\n",
            "24/24 - 0s - loss: 0.1751 - accuracy: 0.9500\n",
            "Epoch 156/200\n",
            "24/24 - 0s - loss: 0.1766 - accuracy: 0.9500\n",
            "Epoch 157/200\n",
            "24/24 - 0s - loss: 0.1785 - accuracy: 0.9417\n",
            "Epoch 158/200\n",
            "24/24 - 0s - loss: 0.1744 - accuracy: 0.9500\n",
            "Epoch 159/200\n",
            "24/24 - 0s - loss: 0.1750 - accuracy: 0.9500\n",
            "Epoch 160/200\n",
            "24/24 - 0s - loss: 0.1767 - accuracy: 0.9417\n",
            "Epoch 161/200\n",
            "24/24 - 0s - loss: 0.1860 - accuracy: 0.9417\n",
            "Epoch 162/200\n",
            "24/24 - 0s - loss: 0.1848 - accuracy: 0.9500\n",
            "Epoch 163/200\n",
            "24/24 - 0s - loss: 0.1922 - accuracy: 0.9500\n",
            "Epoch 164/200\n",
            "24/24 - 0s - loss: 0.1842 - accuracy: 0.9333\n",
            "Epoch 165/200\n",
            "24/24 - 0s - loss: 0.1853 - accuracy: 0.9417\n",
            "Epoch 166/200\n",
            "24/24 - 0s - loss: 0.1714 - accuracy: 0.9333\n",
            "Epoch 167/200\n",
            "24/24 - 0s - loss: 0.1890 - accuracy: 0.9417\n",
            "Epoch 168/200\n",
            "24/24 - 0s - loss: 0.1752 - accuracy: 0.9333\n",
            "Epoch 169/200\n",
            "24/24 - 0s - loss: 0.1751 - accuracy: 0.9500\n",
            "Epoch 170/200\n",
            "24/24 - 0s - loss: 0.1749 - accuracy: 0.9500\n",
            "Epoch 171/200\n",
            "24/24 - 0s - loss: 0.1739 - accuracy: 0.9500\n",
            "Epoch 172/200\n",
            "24/24 - 0s - loss: 0.1701 - accuracy: 0.9500\n",
            "Epoch 173/200\n",
            "24/24 - 0s - loss: 0.1715 - accuracy: 0.9500\n",
            "Epoch 174/200\n",
            "24/24 - 0s - loss: 0.1768 - accuracy: 0.9500\n",
            "Epoch 175/200\n",
            "24/24 - 0s - loss: 0.1714 - accuracy: 0.9417\n",
            "Epoch 176/200\n",
            "24/24 - 0s - loss: 0.1715 - accuracy: 0.9500\n",
            "Epoch 177/200\n",
            "24/24 - 0s - loss: 0.1701 - accuracy: 0.9500\n",
            "Epoch 178/200\n",
            "24/24 - 0s - loss: 0.1705 - accuracy: 0.9500\n",
            "Epoch 179/200\n",
            "24/24 - 0s - loss: 0.1722 - accuracy: 0.9500\n",
            "Epoch 180/200\n",
            "24/24 - 0s - loss: 0.1708 - accuracy: 0.9417\n",
            "Epoch 181/200\n",
            "24/24 - 0s - loss: 0.1722 - accuracy: 0.9500\n",
            "Epoch 182/200\n",
            "24/24 - 0s - loss: 0.1690 - accuracy: 0.9500\n",
            "Epoch 183/200\n",
            "24/24 - 0s - loss: 0.1695 - accuracy: 0.9500\n",
            "Epoch 184/200\n",
            "24/24 - 0s - loss: 0.1714 - accuracy: 0.9500\n",
            "Epoch 185/200\n",
            "24/24 - 0s - loss: 0.1771 - accuracy: 0.9417\n",
            "Epoch 186/200\n",
            "24/24 - 0s - loss: 0.1690 - accuracy: 0.9500\n",
            "Epoch 187/200\n",
            "24/24 - 0s - loss: 0.1712 - accuracy: 0.9500\n",
            "Epoch 188/200\n",
            "24/24 - 0s - loss: 0.1679 - accuracy: 0.9500\n",
            "Epoch 189/200\n",
            "24/24 - 0s - loss: 0.1672 - accuracy: 0.9500\n",
            "Epoch 190/200\n",
            "24/24 - 0s - loss: 0.1772 - accuracy: 0.9333\n",
            "Epoch 191/200\n",
            "24/24 - 0s - loss: 0.1677 - accuracy: 0.9500\n",
            "Epoch 192/200\n",
            "24/24 - 0s - loss: 0.1689 - accuracy: 0.9500\n",
            "Epoch 193/200\n",
            "24/24 - 0s - loss: 0.1676 - accuracy: 0.9417\n",
            "Epoch 194/200\n",
            "24/24 - 0s - loss: 0.1658 - accuracy: 0.9500\n",
            "Epoch 195/200\n",
            "24/24 - 0s - loss: 0.1687 - accuracy: 0.9500\n",
            "Epoch 196/200\n",
            "24/24 - 0s - loss: 0.1716 - accuracy: 0.9417\n",
            "Epoch 197/200\n",
            "24/24 - 0s - loss: 0.1747 - accuracy: 0.9500\n",
            "Epoch 198/200\n",
            "24/24 - 0s - loss: 0.1676 - accuracy: 0.9500\n",
            "Epoch 199/200\n",
            "24/24 - 0s - loss: 0.1670 - accuracy: 0.9417\n",
            "Epoch 200/200\n",
            "24/24 - 0s - loss: 0.1660 - accuracy: 0.9500\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7a8e407ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 0.1255 - accuracy: 1.0000\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7a8daa2950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS5Zdr8zLhyw",
        "outputId": "d1bdb00f-48f1-48d0-ef28-ece13cf5dbe9"
      },
      "source": [
        "# Determine accuracy of voting classifier:\n",
        "voting_accuracy(voting_predictions, test_y)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Voting Classifier Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}