# -*- coding: utf-8 -*-
"""Boston_Hosing_L1_Regularization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MJsBqjxlPgtRmiDQ7hRzbivzw03YWv8Z
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Load the dataset
X, y = datasets.load_boston(return_X_y=True)

# Use only one feature.. Doing this heavily skews the MSE. 
# X = X[:, np.newaxis, 2]

# Split the data into training/testing sets
X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=5,test_size=0.2) #ask dutton about this tomorrow!!

# Old way to split dataset:
# X_train = X[:-20]
# X_test = X[-20:]
# y_train = y[:-20]
# y_test = y[-20:]

# Previous Code:
# # Create linear regression object
# regr = linear_model.LinearRegression()

# New code to do L1 Regularization:
regr = linear_model.Lasso(alpha=0.1)

# Train the model using the training sets
regr.fit(X_train, y_train)

# Make predictions using the testing set
y_pred = regr.predict(X_test)

# Commented out IPython magic to ensure Python compatibility.
# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y_test,y_pred))