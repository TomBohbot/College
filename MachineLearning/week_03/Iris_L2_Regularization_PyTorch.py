# -*- coding: utf-8 -*-
"""Iris_L2_Regularization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sxI4MWYm3GWrXR_jUGDLb_Qa_y49GlT5
"""

# https://veeravignesh1.github.io/post/classification-with-iris-data/
# Inspired by above website: @author Tom Bohbot

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from torch.utils.data import TensorDataset, DataLoader

# Load the dataset:
iris_l = datasets.load_iris()

# Add only 2 characteristics to the dataset, check target == 0 so that it is either a setosa or not a setosa. 
requiredData = {'petal length':iris_l.data[:, 2], 'sepal width': iris_l.data[:, 1], 'target':iris_l.target==0}
df = pd.DataFrame(data = requiredData)

# Split the data into 3 random groups so it can be better tested:
features = df.drop('target',axis=1).values
target = df.target.values
iris = TensorDataset(torch.FloatTensor(features),torch.LongTensor(target))
iris_loader = DataLoader(iris,batch_size=50,shuffle = True)

# Create a class to model each set of 50 data points:
class Model(nn.Module):
    
    def __init__(self,in_features=2,h1=6,h2=3,out_features=3):
        super().__init__()
        self.fc1 = nn.Linear(in_features,h1)
        self.fc2 = nn.Linear(h1,h2)
        self.out = nn.Linear(h2,out_features)
    
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x

mode1 = Model()

# Per requirements, make a train and test set for X and y values:
features = torch.FloatTensor(features)
target = torch.LongTensor(target)
X_train, X_test, y_train, y_test=train_test_split(features,target,random_state=59,test_size=0.2)

# Classification problem => Evaluated based on Cross Entropy Loss
criterion = nn.CrossEntropyLoss()

#Optimizer
optimizer = torch.optim.Adam(mode1.parameters(),lr=0.01)

#Epochs
epochs = 200
losses = []

for i in range(epochs):
    
    ypred = mode1.forward(X_train)
    
    loss = criterion(ypred,y_train)
    #Keeping track of loss
    losses.append(loss.item())

    if i%10==0:
        print(f'Epoch:{i},loss:{loss:.2f}')
        
    #Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Plot the data:
plt.plot(range(epochs),losses)
plt.show()

# Do L2 Regularization right here:
reg = linear_model.Ridge(alpha=.5)
reg.fit(X_train, y_train)

# Set up enviroment to predict unseen data:
with torch.no_grad():
    yval = mode1.forward(X_test)
    loss=criterion(yval,y_test)

# Test the unseen data, with results side by side:
correct =0
for i,data in enumerate(X_test):
    yval = mode1.forward(data)
    
    print(f"Predicted:{yval.argmax()} Actual:{y_test[i]}")
    if yval.argmax().item()== y_test[i]:
        correct+=1
    
print(f"We got {correct} Correct! out of {len(X_test)}")